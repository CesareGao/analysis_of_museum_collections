{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anlysis of museum collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Motivation Slide\n",
    "\n",
    "    - Out dataset is from Metropolitan Museum of Arts Collection. The dataset is in csv format.\n",
    "    - We collect the data by downloading the file from the website https://github.com/metmuseum/openaccess/.\n",
    "    - The meta data was in a mess with many problems such as the data was put in the wrong places, missing values, missing documentation, inconsistent information, possible duplication, mixed text and numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Task Definition / Research Question\n",
    "\n",
    "    -The real-world problem we are interested with is the relation between deparment and collections, what are the types of the object, the relation between years and museum collection activities.\n",
    "        \n",
    "### The relation between deparment and collections\n",
    "    - The input would be the index and the value dataframe of departments from the Department attribute.\n",
    "    - The output would be a plot indicates the relation between the number of occurances and names of departments, i.e. The catalog of the museum collections.\n",
    "\n",
    "### The types of the object\n",
    "    - The input would be the index and the value dataframe of collections from the Object_Name attribute, top ten selections.\n",
    "    - The output would be a plot indicates the top ten types of objects of this museum.\n",
    "\n",
    "### The relation between years and museum collection avtivities\n",
    "    - The input would be the year number from the Credit_Line attribute, and the frequency will be count inside the Series.\n",
    "    - The output would be a plot indicates since the museum has the records, what years does the museum collecting arts from outside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Import all the module required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from math import inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Import the data from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df = pd.read_csv(\"https://media.githubusercontent.com/media/metmuseum/openaccess/master/MetObjects.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data clean\n",
    "\n",
    "The data now looks fine. We intend to do some data clean work as follows:\n",
    "+ Check \"Object Begin Date\" and \"Object End Date\" with \"Object Date\"\n",
    "+ Check \"Artist Begin Date\" and \"Artist End Date\" with \"Artist Display Bio\"\n",
    "+ Check \"Artist Display Name\" with \"Artist Alpha Sort\", then delete the later one.\n",
    "+ Merge \"Artist Role\" with \"Artist Prefix\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Import all the help function we need to use.\n",
    "import data_clean_functions as dcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In original columns, some column names have space in it, which could be difficult to call. So we modify the names of columns for indexing convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df.columns = ['_'.join(col_name.split()) for col_name in collection_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First look of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We could check number of NaNs in all the columns first and try to ingnore those columns which the number of NaNs is no more than 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df =  collection_df.loc[:, collection_df.isna().sum(axis = 0) < len(collection_df.index)*0.1 ]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Check the amount of repository\n",
    "We already know the Metropolitan Museum of Art in New York has only one repository. So we check this in case of spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df.Repository.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Just one row index name. This meets our expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extract the year information in Credit_Line column\n",
    "Extract the credit year from Credit_Line columns and use the year to add an new column called 'Credit_Year' with timeline data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def credit_year(row):\n",
    "    if pd.notna(row):\n",
    "        ## find the year in string type document\n",
    "        year_str = re.compile('\\d{4}').findall(row.split(',')[-1])\n",
    "        if year_str:\n",
    "            year_int = int(year_str[0])\n",
    "            ## filter years according to the found year of the museum\n",
    "            if 1870 <= year_int <= 2019:\n",
    "                return pd.Timestamp(str(year_int))\n",
    "    return np.nan\n",
    "collection_df['Credit_Year'] = collection_df.Credit_Line.apply(credit_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A light check of Artist Display Name\n",
    "The Artist_Display_Name column shoud correspond to the Artist_Alpha_Sort column. These two columns should keep the same content, excepting from abbreviation, alpha sorting, title ignorance.\n",
    "\n",
    "I try to compare the tokenized Artist_Display_Name and Artist_Alpha_Sort column. Compare the percentage of similarity and set the roughly convincing rate to 60% due to abbreviation and title existence. The actually spelling mistake should get more than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def check_name(row):\n",
    "    if pd.isna(row.Artist_Alpha_Sort):\n",
    "        return True\n",
    "    else:\n",
    "        r1 = re.findall(r'\\w+',row.Artist_Display_Name.lower())\n",
    "        r2 = re.findall(r'\\w+',row.Artist_Alpha_Sort.lower())\n",
    "        count = 0\n",
    "        for word in r1:\n",
    "            if word in r2:\n",
    "                count += 1\n",
    "        if count/len(r1) > 0.6:\n",
    "            return True\n",
    "        count = 0\n",
    "        for word in r2:\n",
    "            if word in r1:\n",
    "                count += 1\n",
    "        if count/len(r2) > 0.6:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "collection_df['Artist_Name_Check'] = collection_df[collection_df.Artist_Display_Name.notna()].apply(check_name,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Artist_Role column\n",
    "We intend to check th spelling mistake of Artist_Role column and merge the Artist_Prefix column to it. This should give us a thinking of some most popular roles in history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## expanding contractions\n",
    "contraction_patterns=[(r'can\\'t', 'cannot'),\n",
    "                    (r'haven\\'t', 'have not'),\n",
    "                    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "                    (r'(\\w+)\\'re', '\\g<1> are')]\n",
    "\n",
    "class contraction_replacer(object):\n",
    "    def __init__(self, contraction_patterns):        \n",
    "        # store compiled regex object\n",
    "        self._contraction_regexes = [(re.compile(p), replaced_text) for p, replaced_text in contraction_patterns]\n",
    "        \n",
    "    def do_contraction_normalization(self, text):\n",
    "        for contraction_regex, replaced_text in self._contraction_regexes:\n",
    "            text = contraction_regex.sub(replaced_text,text)\n",
    "        return text\n",
    "\n",
    "def get_noun(row):\n",
    "    sample_contraction_replacer = contraction_replacer(contraction_patterns)\n",
    "    ## word tokenize\n",
    "    if pd.isna(row):\n",
    "        return row\n",
    "    else:\n",
    "        text = re.sub('\\|',' ',row)\n",
    "    words = nltk.tokenize.word_tokenize(sample_contraction_replacer.do_contraction_normalization(text))\n",
    "    words = set(words)\n",
    "\n",
    "    ## stop words removal\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "\n",
    "    ## lemmatization\n",
    "    wnetl = WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if not wordnet.synsets(words[i]):\n",
    "            nword = wnetl.lemmatize(words[i], 'n')\n",
    "            if wordnet.synsets(nword):\n",
    "                words[i] = nword\n",
    "    return '|'.join(words)\n",
    "\n",
    "collection_df.Artist_Role = collection_df.Artist_Role.apply(get_noun,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data analysis & Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the general information of the data set\n",
    "Check the percentage of highlighted collections from the Is_Highlight column and the percentage of public domain collections from the Is_Public_Domain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "highlight_se = collection_df.Is_Highlight.value_counts()\n",
    "quantile_highlight = highlight_se[1]/(highlight_se[0] + highlight_se[1])\n",
    "public_se = collection_df.Is_Public_Domain.value_counts()\n",
    "quantile_public = public_se[1]/(public_se[0] + public_se[1])\n",
    "print(f'The percentage of highlighted collections is {quantile_highlight * 100}%')\n",
    "print(f'The percentage of public domain collections is {quantile_public * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From the results we can find that the highlighted collections is no more than 0.40%. The no more than half of the collections are in public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the possible spelling mistake in Artist_Display_Name\n",
    "This is a really rought check of the possible spelling mistake in Artist_Display_Name columns. Under the pre-condition in data cleaning process, only a few spelling mistake exits, which require human check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(collection_df.Artist_Name_Check.value_counts(dropna=False))\n",
    "collection_df[['Artist_Name_Check','Artist_Display_Name','Artist_Alpha_Sort']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the types of collections\n",
    "The Object_Name column shows the type of each object. We are interest to find the larggest amount of object types in the museum collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "top_ten_collections = collection_df.Object_Name.value_counts(dropna=False).head(10)\n",
    "top_ten_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x = top_ten_collections.values, y = top_ten_collections.index, alpha=0.8, orient='h')\n",
    "plt.title('Top Ten Collection Types in The Museum')\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.ylabel('Collection Types', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the department\n",
    "The Department column shows the department where each object belongs to. We are interest to take a loot at the collections of each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## explore the departement\n",
    "departments = collection_df.Department.value_counts()\n",
    "departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x = departments.values, y = departments.index, alpha=0.8, orient='h')\n",
    "plt.title('Quantiles of Collections in Different Departments')\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.ylabel('Names of Departments', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the frequency of credit line\n",
    "The Credit_Year column we build in data cleaning section shows the the year of the collection been collected. We want to look at the frequency of the credit to check if the frequency increases or decreases with time flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = collection_df.Credit_Year.value_counts()\n",
    "ax = freq_df.plot(figsize = (15,5))\n",
    "freq_df.rolling(window = 30).mean().plot(ax= ax, color='green', label = 'frequnecy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the most popular artist role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def select_top_ten_roles(df):\n",
    "    roles = defaultdict(int)\n",
    "    for i in df[df.Artist_Role.notna()].index:\n",
    "        for role in df.Artist_Role.loc[i].split('|'):\n",
    "            roles[role] += 1\n",
    "    x, y = [], []\n",
    "    upper_bound = inf\n",
    "    for _ in range(10):\n",
    "        lar = -inf\n",
    "        for key,val in roles.items():\n",
    "            if lar < val < upper_bound:\n",
    "                lar = val\n",
    "                temp = key\n",
    "        upper_bound = lar\n",
    "        x.append(lar)\n",
    "        y.append(temp)\n",
    "    return x, y\n",
    "x, y = select_top_ten_roles(collection_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x = x, y = y, alpha=0.8, orient='h')\n",
    "plt.title('Artist Role Count')\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.ylabel('Names of Roles', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analysis conclusion\n",
    "\n",
    "\n",
    "## Top Ten Collection Types in The Museum\n",
    "    - From the plot we can clrearly see that the top ten colection types are print, photograph, drawing, book, fragment, Kylix fragment, piece, painting, negative, Baseball card/paint.\n",
    "    - We use the value and index of the Series to be the x-axis and y-axis number.\n",
    "\n",
    "## Quantiles of Collections in Different Departments\n",
    "    - From the plot we can easily analysis each department's the amount of collection, and we can see the top collection department is Drawings and Paints.\n",
    "    - We also use the value and index of the Series to be the x-axis and y-axis number.\n",
    "\n",
    "## Frequency of Contribution Depend on Years\n",
    "    - From the plot we can state that as year goes, the museum keep collecting arts from people, even though there are some years that have more frequency of collecting activities, the most activities are not changable.\n",
    "    - We first turn the Series into list, then get the year numbers, count the frequency of years appear on the lists, finally plot the year and the frequency value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis conclusion\n",
    "\n",
    "\n",
    "## Top Ten Collection Types in The Museum\n",
    "    - From the plot we can clrearly see that the top ten colection types are print, photograph, drawing, book, fragment, Kylix fragment, piece, painting, negative, Baseball card/paint.\n",
    "    - We use the value and index of the Series to be the x-axis and y-axis number.\n",
    "\n",
    "## Quantiles of Collections in Different Departments\n",
    "    - From the plot we can easily analysis each department's the amount of collection, and we can see the top collection department is Drawings and Paints.\n",
    "    - We also use the value and index of the Series to be the x-axis and y-axis number.\n",
    "\n",
    "## Frequency of Contribution Depend on Years\n",
    "    - From the plot we can state that as year goes, the museum keep collecting arts from people, even though there are some years that have more frequency of collecting activities, the most activities are not changable.\n",
    "    - We first turn the Series into list, then get the year numbers, count the frequency of years appear on the lists, finally plot the year and the frequency value."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
