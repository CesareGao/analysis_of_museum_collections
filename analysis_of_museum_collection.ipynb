{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anlysis of Museum Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset and Motivation\n",
    "\n",
    "- Out dataset is [The Metropolitan Museum of Art Open Access]( https://github.com/metmuseum/openaccess/ ) in CSV format. The Metropolitan Museum of Art has waived all copyright and related or neighboring rights to this dataset to inspire more researchers exploring it.\n",
    "- This dataset include identifying data of more than 470,000 artworks in the museum collection. It provides detailed information of each artworks, which reveal huge potential of data analyzing work.\n",
    "- At the same time, the meta data is full of data cleaning problems, such as the data was put in the wrong places, missing values, missing documentation, inconsistent information, possible duplication, mixed text and numeric data. It is a really good dataset for practicing data cleaning work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main attributes of the dataset are:\n",
    "- **Object Number/Object ID**: a unique number sequence for each object in museum.\n",
    "- **Is Highlight**: a bool to show whether an object is highlighted or not.\n",
    "- **Is Public domain**: a bool to show whether an object is displayed in public domain or not.\n",
    "- **Department**: the department the object belongs to.\n",
    "- **Object name**: how the object called.\n",
    "- **Title**: detailed object name with discription.\n",
    "- **Culture/Period/Dynasty/Reign**: the background of each object.\n",
    "- **Portfolio**: the portfolio the object belongs to.\n",
    "- **Artist Pole/Artist Prefix/Artist Display Name/Artist Display Bio/Artist Suffix/Artist Alpha Sort/Artist Nationality/Artist Begin Date/Artist End Date**: all detailed artist background.\n",
    "- **Object Date/Object Begin Date/Object End Date**: the display period of each object.\n",
    "- **Medium**: the materials using to made the items.\n",
    "- **Dimensions**: the dimension of each object.\n",
    "- **Credit Line**: the credit information of each object.\n",
    "- **Classification**: the category of material each objects belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Research Questions\n",
    "\n",
    "We are interested in the distribution of all the collections, which could be analyzed by storing departments, collection types, credit line history and roles of artist.\n",
    "        \n",
    "- The objects and corresponding departments\n",
    "    - Input: Department name extracted from the Department column.\n",
    "    - Output: A visual check of the number of artworks in each department.\n",
    "\n",
    "- The objects and corresponding types\n",
    "    - Input: Object type extracted from the Object_Name column. This column only contains object type. The detailed discription is stored in Object_Title.\n",
    "    - Output: A visual check of several top amount of artworks' types that are collected in the museum.\n",
    "\n",
    "- The objects and corresponding credit line\n",
    "    - Input: Credit year extracted from the Credit_Line column.\n",
    "    - Output: A visual check of the credit line frequency in the history by time line.\n",
    "- The artist roles\n",
    "    - Input: Artist roled extracted from the Artist_Role column.\n",
    "    - Output: A visual check of most popular roles who produced masterpieces in history. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Literature Reviews\n",
    "\n",
    "An article about data cleaning for museum collection has been found on Google: [Data Cleaning with Python â€” MoMAs Artwork Collection]( https://www.dataquest.io/blog/data-cleaning-with-python )\n",
    "    \n",
    "**What does the author do**\n",
    "\n",
    "    TBD\n",
    "\n",
    "**What do we do**\n",
    "\n",
    "    TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleaning\n",
    "\n",
    "As we talked before, the meta data is full of data cleaning problems. We intend to explore the following problems:\n",
    "\n",
    "+ Check if any spelling mistakes in the Repository column\n",
    "+ Extract the year information in Credit_Line column and store it into an new column called \"Credit_Year\"\n",
    "+ Roughly check if any spelling mistakes in Artist_Display_Name column, which shoud corresponds to the Artist_Alpha_Sort. Then store the judgement into an new column called \"Artist_Name_Check\"\n",
    "+ Check the Artist_Role column, reformatting it and use the reformatted data to replace the raw data.\n",
    "+ Change the datatype of the Metadata_Date columns to datetime data.\n",
    "\n",
    "This data cleaning section is intended to clean the dataset rather than formating the dataset. So in case of losing raw information, we intend to store the cleaned data into new columns and preserve the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Import all the module required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from wordcloud import WordCloud\n",
    "from math import inf\n",
    "import data_clean_functions as dcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Import the data from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df = pd.read_csv(\"https://media.githubusercontent.com/media/metmuseum/openaccess/master/MetObjects.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In original columns, some column names have space in it, which could be difficult to call. So we modify the names of columns for indexing convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df.columns = ['_'.join(col_name.split()) for col_name in collection_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First look of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We could check number of NaNs in all the columns first and try to ingnore those columns whose most values are NaN. Those columns almost filled with contents are more likely to give us general patterns of the dataset, which should be analyzed prior. These kind of columns include Object_Number, Is_Highlight, Is_Public_Domain, Object_ID, Department, Object_Name and etc.\n",
    "\n",
    "We give a practical example of dropping all the columns whose NaN values are more than 10%. But we won't use this for practicing perpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df =  collection_df.loc[:, collection_df.isna().sum(axis = 0) < len(collection_df.index)*0.1 ]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Check the amount of repository\n",
    "We already know the Metropolitan Museum of Art in New York has only one repository. So we check this in case of spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "collection_df.Repository.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Just one row index name. This meets our expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extract the year information in Credit_Line column\n",
    "\n",
    "Extract the credit year from Credit_Line columns and use the year to add an new column called 'Credit_Year' with timeline data type. The credit year is always documented at the end of credit line information, which is easy to extract. The museum is founded in 1870. So we could filter the wrong numebr of years which exceed the [1870,2019] range of time and convert the datatype to timeline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def credit_year(row):\n",
    "    if pd.notna(row):\n",
    "        ## find the year in string type document\n",
    "        year_str = re.compile('\\d{4}').findall(row.split(',')[-1])\n",
    "        if year_str:\n",
    "            year_int = int(year_str[0])\n",
    "            ## filter years according to the found year of the museum\n",
    "            if 1870 <= year_int <= 2019:\n",
    "                return pd.Timestamp(str(year_int))\n",
    "    return np.nan\n",
    "collection_df['Credit_Year'] = collection_df.Credit_Line.apply(credit_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A light check of Artist Display Name\n",
    "The Artist_Display_Name column shoud correspond to the Artist_Alpha_Sort column. These two columns should keep the same content, excepting from abbreviation, alpha sorting, title ignorance.\n",
    "\n",
    "These are really \"dirty\" data, which has lots of problems, like spelling mistakes, full name vs abbreviation, missing titles.\n",
    "\n",
    "We try to compare the tokenized Artist_Display_Name and Artist_Alpha_Sort column. Compare the percentage of similarity and set the roughly convincing rate to 60% due to abbreviation and title existence. The actually spelling mistake should get more than that.\n",
    "\n",
    "Obviously this is a really awkward way to check the names. We actually tried to several other methods which import an named detecting module:\n",
    "\n",
    "+ from nameparser.parser import HumanName\n",
    "+ from nltk.tag.stanford import NERTagger\n",
    "\n",
    "Both of them produced unsatifying results, espetially when applied to the Artist_Alpha_Sort column. There seems no best way to do human name comparation, espetially the comparation between full name and abbreviation. It's time-consuming and ugly to write. So We choose to implement a rough check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def check_name(row):\n",
    "    if pd.isna(row.Artist_Alpha_Sort):\n",
    "        return True\n",
    "    else:\n",
    "        r1 = re.findall(r'\\w+',row.Artist_Display_Name.lower())\n",
    "        r2 = re.findall(r'\\w+',row.Artist_Alpha_Sort.lower())\n",
    "        count = 0\n",
    "        for word in r1:\n",
    "            if word in r2:\n",
    "                count += 1\n",
    "        if count/len(r1) > 0.6:\n",
    "            return True\n",
    "        count = 0\n",
    "        for word in r2:\n",
    "            if word in r1:\n",
    "                count += 1\n",
    "        if count/len(r2) > 0.6:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "collection_df['Artist_Name_Check'] = collection_df[collection_df.Artist_Display_Name.notna()].apply(check_name,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Check the Artist_Role column\n",
    "\n",
    "We intend to check th spelling mistake of Artist_Role column and merge the Artist_Prefix column to it. This should give us a thinking of some most popular roles in history.\n",
    "\n",
    "The data is quite well formatted already. We just need to do some reformatting work. Remove the adjectives and leave the roles only, split the multipal conbined roles and rejoin them.\n",
    "\n",
    "This is definitely extract information by NLP. We ignore some processes as the data is just seperated words rather than sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## expanding contractions\n",
    "contraction_patterns=[(r'can\\'t', 'cannot'),\n",
    "                    (r'haven\\'t', 'have not'),\n",
    "                    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "                    (r'(\\w+)\\'re', '\\g<1> are')]\n",
    "\n",
    "class contraction_replacer(object):\n",
    "    def __init__(self, contraction_patterns):        \n",
    "        # store compiled regex object\n",
    "        self._contraction_regexes = [(re.compile(p), replaced_text) for p, replaced_text in contraction_patterns]\n",
    "        \n",
    "    def do_contraction_normalization(self, text):\n",
    "        for contraction_regex, replaced_text in self._contraction_regexes:\n",
    "            text = contraction_regex.sub(replaced_text,text)\n",
    "        return text\n",
    "\n",
    "def get_noun(row):\n",
    "    sample_contraction_replacer = contraction_replacer(contraction_patterns)\n",
    "    ## word tokenize\n",
    "    if pd.isna(row):\n",
    "        return row\n",
    "    else:\n",
    "        text = re.sub('\\|',' ',row)\n",
    "    words = nltk.tokenize.word_tokenize(sample_contraction_replacer.do_contraction_normalization(text))\n",
    "    words = set(words)\n",
    "\n",
    "    ## stop words removal\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "\n",
    "    ## lemmatization\n",
    "    wnetl = WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if not wordnet.synsets(words[i]):\n",
    "            nword = wnetl.lemmatize(words[i], 'n')\n",
    "            if wordnet.synsets(nword):\n",
    "                words[i] = nword\n",
    "    return '|'.join(words)\n",
    "\n",
    "collection_df.Artist_Role = collection_df.Artist_Role.apply(get_noun,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data analysis & Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the general information of the data set\n",
    "Check the percentage of highlighted collections from the Is_Highlight column and the percentage of public domain collections from the Is_Public_Domain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "highlight_se = collection_df.Is_Highlight.value_counts()\n",
    "quantile_highlight = highlight_se[1]/(highlight_se[0] + highlight_se[1])\n",
    "public_se = collection_df.Is_Public_Domain.value_counts()\n",
    "quantile_public = public_se[1]/(public_se[0] + public_se[1])\n",
    "print(f'The percentage of highlighted collections is {quantile_highlight * 100}%')\n",
    "print(f'The percentage of public domain collections is {quantile_public * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From the results we can find that the highlighted collections is no more than 0.40%. The no more than half of the collections are in public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the possible spelling mistake in Artist_Display_Name\n",
    "This is a really rough check of the possible spelling mistake in Artist_Display_Name columns. Under the pre-condition in data cleaning process, only a few spelling mistake exits, which require human check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(collection_df.Artist_Name_Check.value_counts(dropna=False))\n",
    "collection_df[['Artist_Name_Check','Artist_Display_Name','Artist_Alpha_Sort']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the types of collections\n",
    "The Object_Name column shows the type of each object. We are interested to find the larggest amount of object types in the museum collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "top_ten_collections = collection_df.Object_Name.value_counts(dropna=False).head(10)\n",
    "top_ten_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x = top_ten_collections.values, y = top_ten_collections.index, alpha=0.8, orient='h')\n",
    "plt.title('Top Ten Collection Types in The Museum')\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.ylabel('Collection Types', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the department\n",
    "The Department column shows the department where each object belongs to. We are interested to take a loot at the collections of each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "departments = collection_df.Department.value_counts()\n",
    "departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x = departments.values, y = departments.index, alpha=0.8, orient='h')\n",
    "plt.title('Quantiles of Collections in Different Departments')\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.ylabel('Names of Departments', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the frequency of credit line\n",
    "The Credit_Year column we build in data cleaning section shows the the year of the collection been collected. We want to look at the frequency of the credit to check if the frequency increases or decreases with time flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = collection_df.Credit_Year.value_counts()\n",
    "ax = freq_df.plot(figsize = (15,5))\n",
    "freq_df.rolling(window = 30).mean().plot(ax= ax, color='green', label = 'frequnecy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the most popular artist role\n",
    "\n",
    "The Artist_Roles column is already been cleaned and reformated. We just need to extract them, count the frequency of each role and store it into  a dictionary. There are too many roles, a top ten display should be enough to give some insights. Also a wordnet plot is given for vivid check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def select_top_ten_roles(df):\n",
    "    roles = defaultdict(int)\n",
    "    for i in df[df.Artist_Role.notna()].index:\n",
    "        for role in df.Artist_Role.loc[i].split('|'):\n",
    "            roles[role] += 1\n",
    "    x, y = [], []\n",
    "    upper_bound = inf\n",
    "    for _ in range(10):\n",
    "        lar = -inf\n",
    "        for key,val in roles.items():\n",
    "            if lar < val < upper_bound:\n",
    "                lar = val\n",
    "                temp = key\n",
    "        upper_bound = lar\n",
    "        x.append(lar)\n",
    "        y.append(temp)\n",
    "    return x, y, roles\n",
    "x, y, roles = select_top_ten_roles(collection_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x = x, y = y, alpha=0.8, orient='h')\n",
    "plt.title('Artist Role Count')\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.ylabel('Names of Roles', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = ''\n",
    "for key in roles:\n",
    "    text += (key+' ')*roles[key]\n",
    "# text = \" \".join(words)\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear',)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "**Main Feature**: In this project we are focused on how to clean string data, extract informations from the data bu using regex and NLP. The natural way to analyze the cleaned str data is to count the frequency by category. We tried visualize the frequency both by category and by timeline.\n",
    "\n",
    "**Summary**: The whole project should give others a general look of how the artworks in The Metropolitan Museum of Art distributed with different scales.\n",
    "\n",
    "**Future work**: A further deep analysis could be implemented on the credit year timeline to check if there is any event could be connected with the peak or valley of the credit line frequency. Or we can build a network between the Metropolitan Museum of Art and other museums to see the artworks' flow pattern."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
